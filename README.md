# ZerosApproximator

This sketch animates the method for approximate the zeros of a function using Newton's method.
It was supposed to implement an approximation for the zeros based on gradient descent, that I learned about studying backpropagation in neural networks,
but I found out that, unfortunately, Newton had already tought about that :(

The code is simple and does not need any explanation other than the fact that it relies on the exact derivative of the given function and does not calculate
an approximation for that, but it could be an interesting implementation.

Other implementations may be to actually use the idea to visualize the research of the minimum of the loss function in neural networks using a simplification of the function to a two variables function plotted on a 3D space.
